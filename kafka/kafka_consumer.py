import json
import pickle
import time
import os
import numpy as np
import pandas as pd
from kafka import KafkaConsumer
import mysql.connector
from mysql.connector import Error
import logging
from typing import Dict, Any

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class HappinessKafkaConsumer:
    """
    Consumidor de Kafka para predicci√≥n de Happiness Score en tiempo real.
    
    Proceso ETL:
    1. EXTRACT: Lee mensajes desde Kafka topic
    2. TRANSFORM: Extrae features, aplica modelo ML, calcula m√©tricas
    3. LOAD: Almacena predicciones y resultados en MySQL
    """
    
    def __init__(self,
                 bootstrap_servers: str = 'localhost:9092',
                 topic: str = 'happiness-data',
                 group_id: str = 'happiness-prediction-group',
                 model_path: str = 'modelo_regresion_lineal.pkl',
                 mysql_config: Dict[str, Any] = None):
        """
        Inicializa el consumidor de Kafka.
        
        Args:
            bootstrap_servers: Direcci√≥n del servidor Kafka
            topic: Nombre del topic de Kafka
            group_id: ID del grupo de consumidores
            model_path: Ruta al modelo .pkl guardado
            mysql_config: Configuraci√≥n de conexi√≥n MySQL
        """
        self.topic = topic
        self.model = None
        self.mysql_config = mysql_config or self._default_mysql_config()
        self.model_reload_attempts = 0  # Contador de intentos de recarga
        
        # Cargar modelo
        self._load_model(model_path)
        
        # Configurar consumidor de Kafka
        try:
            self.consumer = KafkaConsumer(
                topic,
                bootstrap_servers=bootstrap_servers,
                group_id=group_id,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                auto_offset_reset='earliest',  # Leer desde el inicio
                enable_auto_commit=True,
                auto_commit_interval_ms=1000,
                max_poll_records=10
            )
            logger.info(f"‚úÖ Consumidor Kafka inicializado: {bootstrap_servers}")
            logger.info(f"üì• Suscrito al topic: {topic}")
        except Exception as e:
            logger.error(f"‚ùå Error al inicializar consumidor Kafka: {e}")
            raise
        
        # Configurar conexi√≥n MySQL
        self._setup_mysql_connection()
        self._create_predictions_table()
    
    def _default_mysql_config(self) -> Dict[str, Any]:
        """Retorna configuraci√≥n por defecto de MySQL"""
        return {
            'host': 'localhost',
            'port': 3306,
            'database': 'happiness_db',
            'user': 'root',
            'password': 'root'
        }
    
    def _load_model(self, model_path: str) -> None:
        """
        Carga el modelo ML desde un archivo .pkl (contiene modelo + preprocessor).
        
        Args:
            model_path: Ruta al archivo del modelo
        """
        try:
            with open(model_path, 'rb') as f:
                self.model = pickle.load(f)
            
            # Verificar que el modelo se carg√≥ correctamente
            if self.model is None:
                logger.error(f"‚ùå El modelo cargado es None")
                raise ValueError("Modelo cargado es None")
            
            # Verificar que es un diccionario con 'modelo' y 'preprocessor'
            if not isinstance(self.model, dict):
                logger.error(f"‚ùå El modelo debe ser un diccionario con 'modelo' y 'preprocessor'")
                raise ValueError("Modelo inv√°lido: formato incorrecto")
            
            if 'modelo' not in self.model or 'preprocessor' not in self.model:
                logger.error(f"‚ùå El modelo debe contener keys 'modelo' y 'preprocessor'")
                raise ValueError("Modelo inv√°lido: faltan componentes")
            
            # Verificar que el modelo tiene el m√©todo predict
            if not hasattr(self.model['modelo'], 'predict'):
                logger.error(f"‚ùå El modelo no tiene m√©todo 'predict'")
                raise ValueError("Modelo inv√°lido: no tiene m√©todo 'predict'")
            
            logger.info(f"‚úÖ Modelo cargado exitosamente desde {model_path}")
            logger.info(f"   Tipo de modelo: {type(self.model['modelo']).__name__}")
            logger.info(f"   Preprocessor: {type(self.model['preprocessor']).__name__}")
            
        except FileNotFoundError:
            logger.error(f"‚ùå Archivo de modelo no encontrado: {model_path}")
            logger.info("üí° Por favor, ejecuta primero: python model_regresion/model_utils.py")
            self.model = None
            raise
        except Exception as e:
            logger.error(f"‚ùå Error al cargar modelo: {e}")
            self.model = None
            raise
    
    def _save_current_model(self, model_path: str) -> None:
        """Guarda el modelo actual (placeholder para generar .pkl si no existe)"""
        logger.warning("‚ö†Ô∏è Por favor, guarda tu modelo entrenado como .pkl")
        logger.info("üí° Ejemplo: pickle.dump(modelo_lr, open('modelo_regresion_lineal.pkl', 'wb'))")
    
    def _setup_mysql_connection(self) -> None:
        """Configura la conexi√≥n a MySQL y crea la base de datos si no existe"""
        try:
            # Primero conectar sin especificar base de datos para crearla
            config_without_db = self.mysql_config.copy()
            database_name = config_without_db.pop('database')
            
            # Conectar a MySQL sin BD
            temp_conn = mysql.connector.connect(**config_without_db)
            cursor = temp_conn.cursor()
            
            # Crear base de datos si no existe
            cursor.execute(f"CREATE DATABASE IF NOT EXISTS {database_name}")
            logger.info(f"‚úÖ Base de datos '{database_name}' verificada/creada")
            
            cursor.close()
            temp_conn.close()
            
            # Ahora conectar a la base de datos espec√≠fica
            self.mysql_conn = mysql.connector.connect(**self.mysql_config)
            if self.mysql_conn.is_connected():
                logger.info(f"‚úÖ Conectado a MySQL: {self.mysql_config['database']}")
        except Error as e:
            logger.error(f"‚ùå Error al conectar a MySQL: {e}")
            logger.info("üí° Aseg√∫rate de que MySQL est√° corriendo y las credenciales son correctas")
            raise
    
    def _create_predictions_table(self) -> None:
        """Crea la tabla de predicciones si no existe"""
        create_table_query = """
        CREATE TABLE IF NOT EXISTS predictions (
            record_id INT AUTO_INCREMENT PRIMARY KEY,
            country VARCHAR(100),
            year INT,
            
            -- Caracter√≠sticas (Features)
            gdp_per_capita FLOAT,
            social_support FLOAT,
            healthy_life_expectancy FLOAT,
            freedom_to_make_life_choices FLOAT,
            generosity FLOAT,
            perceptions_of_corruption FLOAT,
            
            -- Scores
            actual_score FLOAT,
            predicted_score FLOAT,
            prediction_error FLOAT,
            
            -- Metadata
            type_model VARCHAR(20),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            INDEX idx_country (country),
            INDEX idx_year (year),
            INDEX idx_type_model (type_model),
            INDEX idx_created_at (created_at)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """
        
        try:
            cursor = self.mysql_conn.cursor()
            cursor.execute(create_table_query)
            self.mysql_conn.commit()
            cursor.close()
            logger.info("‚úÖ Tabla 'predictions' verificada/creada")
        except Error as e:
            logger.error(f"‚ùå Error al crear tabla: {e}")
            raise
    
    # =========================================================================
    # EXTRACT: Extracci√≥n de datos desde Kafka
    # =========================================================================
    
    def extract_from_kafka_message(self, message) -> Dict[str, Any]:
        """
        [ETL - EXTRACT] Extrae el registro completo del mensaje de Kafka.
        
        Args:
            message: Mensaje de Kafka
            
        Returns:
            Diccionario con los datos del registro
        """
        record = message.value
        logger.debug(f"üì• [EXTRACT] Mensaje recibido: Record ID {record['record_id']}")
        return record
    
    # =========================================================================
    # TRANSFORM: Transformaci√≥n y procesamiento de datos
    # =========================================================================
    
    def transform_extract_features(self, record: Dict[str, Any]) -> pd.DataFrame:
        """
        [ETL - TRANSFORM] Extrae y ordena las caracter√≠sticas para el modelo.
        
        Args:
            record: Registro del mensaje de Kafka
            
        Returns:
            DataFrame con las 6 caracter√≠sticas num√©ricas + Country (para One-Hot Encoding)
        """
        features = record['features']
        
        # Crear DataFrame con el orden correcto (6 num√©ricas + Country)
        # IMPORTANTE: El preprocessor del modelo aplicar√° One-Hot Encoding a Country
        feature_df = pd.DataFrame({
            'GDP per capita': [features['GDP_per_capita']],
            'Social support': [features['Social_support']],
            'Healthy life expectancy': [features['Healthy_life_expectancy']],
            'Freedom to make life choices': [features['Freedom_to_make_life_choices']],
            'Generosity': [features['Generosity']],
            'Perceptions of corruption': [features['Perceptions_of_corruption']],
            'Country': [features['Country']]  # ‚úÖ A√ëADIDO: Variable categ√≥rica
        })
        
        return feature_df
    
    def transform_predict_score(self, features: pd.DataFrame) -> float:
        """
        [ETL - TRANSFORM] Aplica el modelo ML para predecir el Happiness Score.
        
        Args:
            features: DataFrame con caracter√≠sticas (6 num√©ricas + Country)
            
        Returns:
            Score predicho por el modelo
        """
        try:
            # Verificar que el modelo existe y tiene preprocessor
            if self.model is None:
                if self.model_reload_attempts < 3:
                    self.model_reload_attempts += 1
                    logger.warning(f"‚ö†Ô∏è Modelo no est√° cargado. Intento de recarga #{self.model_reload_attempts}...")
                    try:
                        self._load_model('modelo_regresion_lineal.pkl')
                        if self.model is not None:
                            self.model_reload_attempts = 0  # Resetear contador si √©xito
                    except:
                        pass
                
                if self.model is None:
                    logger.error(f"‚ùå No se pudo recargar el modelo despu√©s de {self.model_reload_attempts} intentos")
                    return 0.0
            
            # El modelo es un diccionario con 'modelo' y 'preprocessor'
            modelo = self.model['modelo']
            preprocessor = self.model['preprocessor']
            
            # Aplicar preprocessor (One-Hot Encoding a Country)
            features_transformed = preprocessor.transform(features)
            
            # Realizar predicci√≥n
            prediction = modelo.predict(features_transformed)[0]
            return float(prediction)
            
        except Exception as e:
            logger.error(f"‚ùå Error en predicci√≥n: {e}")
            logger.error(f"   Tipo de modelo: {type(self.model)}")
            logger.error(f"   Features type: {type(features)}")
            if isinstance(features, pd.DataFrame):
                logger.error(f"   Features columns: {features.columns.tolist()}")
            return 0.0
    
    def transform_calculate_metrics(self, actual_score: float, predicted_score: float) -> Dict[str, float]:
        """
        [ETL - TRANSFORM] Calcula m√©tricas de error de la predicci√≥n.
        
        Args:
            actual_score: Score real
            predicted_score: Score predicho
            
        Returns:
            Diccionario con m√©tricas calculadas
        """
        metrics = {
            'prediction_error': abs(actual_score - predicted_score),
            'squared_error': (actual_score - predicted_score) ** 2,
            'percentage_error': abs((actual_score - predicted_score) / actual_score) * 100 if actual_score != 0 else 0
        }
        logger.debug(f"üìä [TRANSFORM] M√©tricas calculadas: Error={metrics['prediction_error']:.4f}")
        return metrics
    
    # =========================================================================
    # LOAD: Carga de datos a MySQL y CSV
    # =========================================================================
    
    def load_to_mysql(self, record: Dict[str, Any], 
                      predicted_score: float,
                      type_model: str) -> None:
        """
        [ETL - LOAD] Persiste el registro, predicci√≥n y m√©tricas en MySQL.
        Normaliza nombres de pa√≠ses antes de insertar.
        
        Args:
            record: Registro original desde Kafka
            predicted_score: Score predicho por el modelo
            type_model: Tipo de conjunto de datos ('train' o 'test')
        """
        insert_query = """
        INSERT INTO predictions (
            country, year,
            gdp_per_capita, social_support, healthy_life_expectancy,
            freedom_to_make_life_choices, generosity, perceptions_of_corruption,
            actual_score, predicted_score, prediction_error, type_model
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
        """
        
        features = record['features']
        actual_score = record['actual_score']
        prediction_error = abs(actual_score - predicted_score)
        
        # Obtener pa√≠s del mensaje de Kafka y normalizar
        country = record['country']
        # Unificar Somaliland region -> Somalia
        if 'Somaliland' in country:
            country = 'Somalia'
        
        values = (
            country,
            record['year'],
            features['GDP_per_capita'],
            features['Social_support'],
            features['Healthy_life_expectancy'],
            features['Freedom_to_make_life_choices'],
            features['Generosity'],
            features['Perceptions_of_corruption'],
            actual_score,
            predicted_score,
            prediction_error,
            type_model
        )
        
        try:
            cursor = self.mysql_conn.cursor()
            cursor.execute(insert_query, values)
            self.mysql_conn.commit()
            cursor.close()
            logger.debug(f"üíæ Registro guardado en MySQL")
        except Error as e:
            logger.error(f"‚ùå Error al guardar en MySQL: {e}")
            self.mysql_conn.rollback()
    
    def load_to_csv(self, csv_filename: str = 'predictions_streaming.csv') -> None:
        """
        [ETL - LOAD] Exporta todos los datos de MySQL a un archivo CSV en la carpeta data.
        
        Args:
            csv_filename: Nombre del archivo CSV a generar
        """
        import pandas as pd
        
        # Determinar ruta del archivo CSV
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        data_dir = os.path.join(project_root, 'data')
        
        # Crear directorio si no existe
        os.makedirs(data_dir, exist_ok=True)
        
        csv_path = os.path.join(data_dir, csv_filename)
        
        try:
            logger.info("üì§ [LOAD] Exportando datos a CSV...")
            
            # Consulta para obtener todos los datos
            query = """
            SELECT 
                country, region, year,
                gdp_per_capita, social_support, healthy_life_expectancy,
                freedom_to_make_life_choices, generosity, perceptions_of_corruption,
                actual_score, predicted_score, prediction_error,
                type_model, created_at
            FROM predictions
            ORDER BY created_at
            """
            
            # Cargar datos desde MySQL
            df = pd.read_sql(query, self.mysql_conn)
            
            # Guardar a CSV
            df.to_csv(csv_path, index=False)
            
            logger.info(f"‚úÖ [LOAD] Datos exportados exitosamente")
            logger.info(f"   üìÅ Archivo: {csv_path}")
            logger.info(f"   üìä Registros: {len(df)}")
            
            # Mostrar estad√≠sticas por type_model
            if 'type_model' in df.columns:
                split_counts = df['type_model'].value_counts()
                logger.info(f"   üìà Distribuci√≥n:")
                for split, count in split_counts.items():
                    logger.info(f"      - {split}: {count} registros")
            
        except Exception as e:
            logger.error(f"‚ùå [LOAD] Error al exportar CSV: {e}")
    
    # =========================================================================
    # PROCESO ETL COMPLETO
    # =========================================================================
    
    def run_etl_on_message(self, message) -> None:
        """
        [ETL PIPELINE] Ejecuta el pipeline completo Extract ‚Üí Transform ‚Üí Load por mensaje.
        
        Proceso:
        1. EXTRACT: Lee mensaje desde Kafka
        2. TRANSFORM: Extrae features, predice score, calcula m√©tricas
        3. LOAD: Guarda predicci√≥n en MySQL
        
        Args:
            message: Mensaje de Kafka
        """
        try:
            # ==================== EXTRACT ====================
            record = self.extract_from_kafka_message(message)
            
            # ==================== TRANSFORM ====================
            # Transformaci√≥n 1: Extraer features
            features = self.transform_extract_features(record)
            
            # Transformaci√≥n 2: Predecir score
            predicted_score = self.transform_predict_score(features)
            
            # Transformaci√≥n 3: Calcular m√©tricas
            metrics = self.transform_calculate_metrics(
                record['actual_score'], 
                predicted_score
            )
            
            # Obtener type_model del registro (viene del producer)
            type_model = record.get('type_model', 'unknown')
            
            # ==================== LOAD ====================
            self.load_to_mysql(record, predicted_score, type_model)
            
            # Log de resultado
            logger.info(
                f"‚úÖ [ETL] Record #{record['record_id']}: "
                f"{record['country']} ({record['year']}) | "
                f"Type: {type_model} | "
                f"Real: {record['actual_score']:.2f} | "
                f"Predicho: {predicted_score:.2f} | "
                f"Error: {metrics['prediction_error']:.2f}"
            )
            
        except Exception as e:
            logger.error(f"‚ùå [ETL] Error al procesar mensaje: {e}")
    
    def start_etl_streaming(self, timeout_ms: int = 1000) -> None:
        """
        Inicia el procesamiento ETL en streaming de mensajes desde Kafka.
        
        Args:
            timeout_ms: Timeout para poll de mensajes
        """
        logger.info("="*80)
        logger.info("ÔøΩ INICIANDO PIPELINE ETL - CONSUMER")
        logger.info("="*80)
        logger.info("üöÄ Consumiendo mensajes desde Kafka...")
        logger.info("‚è∏Ô∏è  Presiona Ctrl+C para detener")
        logger.info("="*80)
        
        try:
            messages_processed = 0
            train_processed = 0
            test_processed = 0
            total_error = 0.0
            
            for message in self.consumer:
                # Ejecutar ETL pipeline por mensaje
                self.run_etl_on_message(message)
                
                # Actualizar estad√≠sticas
                messages_processed += 1
                record = message.value
                type_model = record.get('type_model', 'unknown')
                
                if type_model == 'train':
                    train_processed += 1
                elif type_model == 'test':
                    test_processed += 1
                
                # Log cada 10 mensajes
                if messages_processed % 10 == 0:
                    logger.info(
                        f"\nüìä [ESTAD√çSTICAS] Total procesados: {messages_processed} | "
                        f"Train: {train_processed} | Test: {test_processed}\n"
                    )
                
        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è Consumo interrumpido por usuario")
        except Exception as e:
            logger.error(f"‚ùå Error en consumo: {e}")
            raise
        finally:
            logger.info("\n" + "="*80)
            logger.info("‚úÖ PIPELINE ETL FINALIZADO")
            logger.info("="*80)
            logger.info(f"üìä Total mensajes procesados: {messages_processed}")
            logger.info(f"   - Train: {train_processed}")
            logger.info(f"   - Test: {test_processed}")
            logger.info("="*80)
            
            # Exportar datos a CSV antes de cerrar
            if messages_processed > 0:
                logger.info("\nüì• Exportando datos procesados a CSV...")
                self.load_to_csv()
            
            self.close()
    
    def close(self):
        """Cierra las conexiones de Kafka y MySQL"""
        try:
            self.consumer.close()
            logger.info("üîí Consumidor Kafka cerrado")
        except Exception as e:
            logger.error(f"‚ùå Error al cerrar consumidor: {e}")
        
        try:
            if self.mysql_conn.is_connected():
                self.mysql_conn.close()
                logger.info("üîí Conexi√≥n MySQL cerrada")
        except Exception as e:
            logger.error(f"‚ùå Error al cerrar MySQL: {e}")


# =============================================================================
# FUNCI√ìN PRINCIPAL
# =============================================================================

def main():
    """Funci√≥n principal para ejecutar el consumidor"""
    
    # Configuraci√≥n Kafka
    KAFKA_SERVER = 'localhost:9092'
    TOPIC = 'happiness-data'
    GROUP_ID = 'happiness-prediction-group'
    
    # Ruta absoluta al modelo
    script_dir = os.path.dirname(os.path.abspath(__file__))  # kafka/
    project_root = os.path.dirname(script_dir)  # Workshop 3/
    MODEL_PATH = os.path.join(project_root, 'model_regresion', 'modelo_regresion_lineal.pkl')
    
    # Configuraci√≥n MySQL
    MYSQL_CONFIG = {
        'host': 'localhost',
        'port': 3306,
        'database': 'happiness_db',
        'user': 'root',
        'password': 'root'  # ‚ö†Ô∏è Cambia esto por tu contrase√±a de MySQL
    }
    
    # Crear consumidor
    consumer = HappinessKafkaConsumer(
        bootstrap_servers=KAFKA_SERVER,
        topic=TOPIC,
        group_id=GROUP_ID,
        model_path=MODEL_PATH,
        mysql_config=MYSQL_CONFIG
    )
    
    # Iniciar pipeline ETL streaming
    consumer.start_etl_streaming()


if __name__ == "__main__":
    print("="*80)
    print("üöÄ KAFKA CONSUMER - World Happiness Report ML System")
    print("="*80)
    main()
